{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca3f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bebcdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('EXP1.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ef24f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[14.18278985, 24.85084955, 32.26250018, ..., 10.00417662,\n",
       "          7.46888704,  3.62294237],\n",
       "        [12.57967283, 23.76741535, 30.37286871, ..., 14.95723319,\n",
       "         11.13698112,  5.84942905],\n",
       "        [13.22193417, 24.55529474, 30.39580222, ..., 13.83155641,\n",
       "         10.59001351,  5.29500675],\n",
       "        ...,\n",
       "        [13.29084536, 24.17527752, 31.31167992, ..., 14.23324907,\n",
       "         10.53464287,  4.58407225],\n",
       "        [12.95507451, 24.79821619, 31.58751678, ..., 15.15993757,\n",
       "         11.82398723,  5.91199362],\n",
       "        [12.83678016, 22.97733591, 30.31822964, ...,  9.57554661,\n",
       "          7.2255416 ,  3.23136044]]),\n",
       " array([ 3, 14,  9, ...,  4, 14, 15]),\n",
       " array([[13.28076564, 22.61852359, 31.40274702, ..., 10.60715627,\n",
       "          7.27644917,  3.63822458],\n",
       "        [12.94003695, 25.86111181, 29.316574  , ..., 10.92564205,\n",
       "          8.02663748,  4.04376155],\n",
       "        [14.43351816, 25.80163901, 33.27687417, ..., 11.81740281,\n",
       "          9.40164209,  3.67045161],\n",
       "        ...,\n",
       "        [ 9.31385356, 16.99185373, 21.98111139, ...,  8.65813213,\n",
       "          5.74999087,  2.86072738],\n",
       "        [10.12595963, 18.25930482, 23.00005503, ...,  9.45903101,\n",
       "          5.88334479,  3.65900747],\n",
       "        [ 9.184745  , 17.25962422, 21.51717653, ...,  8.96689396,\n",
       "          5.68277104,  2.87023281]]),\n",
       " array([ 0,  0,  0, ..., 15, 15, 15])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a06b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 120)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f10f77c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data[0], columns=[f\"Feature_{i}\" for i in range(1, 121)])\n",
    "label = pd.DataFrame(data[1], columns=['Titik'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a36085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df, label, test_size=0.3, random_state=142, stratify=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7055c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "846a908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class MyCallback(Callback):\n",
    "    def __init__(self, threshold=0.80):\n",
    "        super(MyCallback, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_accuracy = logs.get('val_sparse_categorical_accuracy')\n",
    "        if val_accuracy is not None and val_accuracy >= self.threshold:\n",
    "            print(f\"\\nAkurasi telah mencapai {self.threshold*100}% di epoch {epoch}, menghentikan pelatihan...\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# class MyCallback(Callback):\n",
    "#     def __init__(self, threshold=0.80):\n",
    "#         super(MyCallback, self).__init__()\n",
    "#         self.threshold = threshold\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         # Cek apakah akurasi validasi telah melebihi threshold\n",
    "#         if logs.get('val_accuracy') >= self.threshold:\n",
    "#             print(f\"\\nAkurasi telah mencapai {self.threshold*100}% di epoch {epoch}, menghentikan pelatihan...\")\n",
    "#             self.model.stop_training = True\n",
    "\n",
    "# Membuat instance dari MyCallback\n",
    "my_callback = MyCallback(threshold=0.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2ff17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# LSTM\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(120, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(16, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55dd6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neural_structured_learning as nsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dc95c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_config = nsl.configs.make_adv_reg_config(multiplier=0.2, adv_step_size=0.05)\n",
    "adv_model = nsl.keras.AdversarialRegularization(model, adv_config=adv_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f96cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d36bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Cannot perturb non-Tensor input: dict_keys(['label'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 37s 790ms/step - loss: 2.5765 - sparse_categorical_crossentropy: 2.1454 - sparse_categorical_accuracy: 0.3127 - scaled_adversarial_loss: 0.4311 - val_loss: 3.2710 - val_sparse_categorical_crossentropy: 2.7257 - val_sparse_categorical_accuracy: 0.0792 - val_scaled_adversarial_loss: 0.5453\n",
      "Epoch 2/100\n",
      "35/35 [==============================] - 20s 566ms/step - loss: 1.7270 - sparse_categorical_crossentropy: 1.4336 - sparse_categorical_accuracy: 0.5462 - scaled_adversarial_loss: 0.2933 - val_loss: 3.2390 - val_sparse_categorical_crossentropy: 2.6986 - val_sparse_categorical_accuracy: 0.1245 - val_scaled_adversarial_loss: 0.5404\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - 18s 524ms/step - loss: 1.2144 - sparse_categorical_crossentropy: 1.0090 - sparse_categorical_accuracy: 0.6842 - scaled_adversarial_loss: 0.2054 - val_loss: 2.5995 - val_sparse_categorical_crossentropy: 2.1648 - val_sparse_categorical_accuracy: 0.2677 - val_scaled_adversarial_loss: 0.4347\n",
      "Epoch 4/100\n",
      "35/35 [==============================] - 21s 596ms/step - loss: 0.8666 - sparse_categorical_crossentropy: 0.7210 - sparse_categorical_accuracy: 0.7788 - scaled_adversarial_loss: 0.1457 - val_loss: 2.3913 - val_sparse_categorical_crossentropy: 1.9901 - val_sparse_categorical_accuracy: 0.3281 - val_scaled_adversarial_loss: 0.4011\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - 20s 555ms/step - loss: 0.6821 - sparse_categorical_crossentropy: 0.5679 - sparse_categorical_accuracy: 0.8248 - scaled_adversarial_loss: 0.1141 - val_loss: 2.0885 - val_sparse_categorical_crossentropy: 1.7367 - val_sparse_categorical_accuracy: 0.4365 - val_scaled_adversarial_loss: 0.3518\n",
      "Epoch 6/100\n",
      "35/35 [==============================] - 19s 534ms/step - loss: 0.5039 - sparse_categorical_crossentropy: 0.4188 - sparse_categorical_accuracy: 0.8712 - scaled_adversarial_loss: 0.0851 - val_loss: 0.9120 - val_sparse_categorical_crossentropy: 0.7562 - val_sparse_categorical_accuracy: 0.7250 - val_scaled_adversarial_loss: 0.1558\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - 18s 524ms/step - loss: 0.3633 - sparse_categorical_crossentropy: 0.3005 - sparse_categorical_accuracy: 0.9100 - scaled_adversarial_loss: 0.0627 - val_loss: 0.8735 - val_sparse_categorical_crossentropy: 0.7235 - val_sparse_categorical_accuracy: 0.7609 - val_scaled_adversarial_loss: 0.1500\n",
      "Epoch 8/100\n",
      "35/35 [==============================] - 18s 525ms/step - loss: 0.2710 - sparse_categorical_crossentropy: 0.2234 - sparse_categorical_accuracy: 0.9362 - scaled_adversarial_loss: 0.0476 - val_loss: 0.4851 - val_sparse_categorical_crossentropy: 0.4009 - val_sparse_categorical_accuracy: 0.8536 - val_scaled_adversarial_loss: 0.0842\n",
      "Epoch 9/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2051 - sparse_categorical_crossentropy: 0.1684 - sparse_categorical_accuracy: 0.9520 - scaled_adversarial_loss: 0.0366\n",
      "Akurasi telah mencapai 94.0% di epoch 8, menghentikan pelatihan...\n",
      "35/35 [==============================] - 18s 528ms/step - loss: 0.2051 - sparse_categorical_crossentropy: 0.1684 - sparse_categorical_accuracy: 0.9520 - scaled_adversarial_loss: 0.0366 - val_loss: 0.2298 - val_sparse_categorical_crossentropy: 0.1894 - val_sparse_categorical_accuracy: 0.9401 - val_scaled_adversarial_loss: 0.0404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14133926be0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_model.fit({'feature': X_train, 'label': y_train}, validation_data={'feature': X_val, 'label': y_val}, batch_size=128, epochs=100,callbacks=[my_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a305ffc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salma\\anaconda3\\envs\\gpu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\salma\\anaconda3\\envs\\gpu\\lib\\site-packages\\art\\estimators\\certification\\__init__.py:29: UserWarning: PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\n",
      "  warnings.warn(\"PyTorch not found. Not importing DeepZ or Interval Bound Propagation functionality\")\n"
     ]
    }
   ],
   "source": [
    "from art.attacks.evasion import ProjectedGradientDescent\n",
    "from art.estimators.classification import TensorFlowV2Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33ce6a6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"AdversarialRegularization\" \"                 f\"(type AdversarialRegularization).\n\nCannot convert 'label' to EagerTensor of dtype float\n\nCall arguments received by layer \"AdversarialRegularization\" \"                 f\"(type AdversarialRegularization):\n  • inputs=tf.Tensor(shape=(32, 120), dtype=float32)\n  • kwargs={'training': 'False'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Kemudian, buat serangan PGD dan terapkan pada X_test\u001b[39;00m\n\u001b[0;32m     10\u001b[0m attack \u001b[38;5;241m=\u001b[39m ProjectedGradientDescent(classifier, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, eps_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m x_test_adv \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Evaluasi model yang belum dibungkus pada contoh adversarial yang dihasilkan\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_val, y_val)\n",
      "File \u001b[1;32mc:\\Users\\salma\\anaconda3\\envs\\gpu\\lib\\site-packages\\art\\attacks\\evasion\\projected_gradient_descent\\projected_gradient_descent.py:200\u001b[0m, in \u001b[0;36mProjectedGradientDescent.generate\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03mGenerate adversarial samples and return them in an array.\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m:return: An array holding the adversarial examples.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    199\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating adversarial samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attack\u001b[38;5;241m.\u001b[39mgenerate(x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\salma\\anaconda3\\envs\\gpu\\lib\\site-packages\\art\\attacks\\evasion\\projected_gradient_descent\\projected_gradient_descent_tensorflow_v2.py:152\u001b[0m, in \u001b[0;36mProjectedGradientDescentTensorFlowV2.generate\u001b[1;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_eps()\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Set up targets\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Create dataset\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Here we need to make a distinction: if the masks are different for each input, we need to index\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# those for the current batch. Otherwise (i.e. mask is meant to be broadcasted), keep it as it is.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\salma\\anaconda3\\envs\\gpu\\lib\\site-packages\\art\\attacks\\evasion\\projected_gradient_descent\\projected_gradient_descent_numpy.py:171\u001b[0m, in \u001b[0;36mProjectedGradientDescentCommon._set_targets\u001b[1;34m(self, x, y, classifier_mixin)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# Use model predictions as correct outputs\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classifier_mixin:\n\u001b[1;32m--> 171\u001b[0m     targets \u001b[38;5;241m=\u001b[39m get_labels_np_array(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mpredict(x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n",
      "File \u001b[1;32mc:\\Users\\salma\\anaconda3\\envs\\gpu\\lib\\site-packages\\art\\estimators\\classification\\classifier.py:73\u001b[0m, in \u001b[0;36mInputFilter.__init__.<locals>.make_replacement.<locals>.replacement_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     72\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(lst)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fdict[func_name](\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\salma\\anaconda3\\envs\\gpu\\lib\\site-packages\\art\\estimators\\classification\\tensorflow.py:930\u001b[0m, in \u001b[0;36mTensorFlowV2Classifier.predict\u001b[1;34m(self, x, batch_size, training_mode, **kwargs)\u001b[0m\n\u001b[0;32m    924\u001b[0m     begin, end \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    925\u001b[0m         m \u001b[38;5;241m*\u001b[39m batch_size,\n\u001b[0;32m    926\u001b[0m         \u001b[38;5;28mmin\u001b[39m((m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size, x_preprocessed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]),\n\u001b[0;32m    927\u001b[0m     )\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;66;03m# Run prediction\u001b[39;00m\n\u001b[1;32m--> 930\u001b[0m     results_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_preprocessed\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_mode\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    932\u001b[0m results \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack(results_list)\n\u001b[0;32m    934\u001b[0m \u001b[38;5;66;03m# Apply postprocessing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\salma\\anaconda3\\envs\\gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\salma\\anaconda3\\envs\\gpu\\lib\\site-packages\\neural_structured_learning\\keras\\adversarial_regularization.py:668\u001b[0m, in \u001b[0;36mAdversarialRegularization.call\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 668\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_keys\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;66;03m# This is to prevent \"no loss to optimize\" error when the first call to\u001b[39;00m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;66;03m# the model is without label input.\u001b[39;00m\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels are not in the input. For predicting examples \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    672\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwithout labels, please use the base model instead.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    674\u001b[0m   labels, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_labels_and_weights(inputs)\n",
      "File \u001b[1;32mc:\\Users\\salma\\anaconda3\\envs\\gpu\\lib\\site-packages\\neural_structured_learning\\keras\\adversarial_regularization.py:668\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 668\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_keys):\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;66;03m# This is to prevent \"no loss to optimize\" error when the first call to\u001b[39;00m\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;66;03m# the model is without label input.\u001b[39;00m\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels are not in the input. For predicting examples \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    672\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwithout labels, please use the base model instead.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    674\u001b[0m   labels, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_labels_and_weights(inputs)\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling layer \"AdversarialRegularization\" \"                 f\"(type AdversarialRegularization).\n\nCannot convert 'label' to EagerTensor of dtype float\n\nCall arguments received by layer \"AdversarialRegularization\" \"                 f\"(type AdversarialRegularization):\n  • inputs=tf.Tensor(shape=(32, 120), dtype=float32)\n  • kwargs={'training': 'False'}"
     ]
    }
   ],
   "source": [
    "classifier = TensorFlowV2Classifier(\n",
    "    model=adv_model,  # model asli, bukan adv_model\n",
    "    nb_classes=len(np.unique(y_train)),\n",
    "    input_shape=(len(X_train[0]),),\n",
    "    loss_object=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    clip_values=(X_val.min(), X_val.max())\n",
    ")\n",
    "\n",
    "# Kemudian, buat serangan PGD dan terapkan pada X_test\n",
    "attack = ProjectedGradientDescent(classifier, eps=0.2, eps_step=0.01, max_iter=40)\n",
    "x_test_adv = attack.generate(X_val)\n",
    "\n",
    "# Evaluasi model yang belum dibungkus pada contoh adversarial yang dihasilkan\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Akurasi pada contoh adversarial: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd1de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
