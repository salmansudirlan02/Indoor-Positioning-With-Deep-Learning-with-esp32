{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda96f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f04fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('EXP1.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9801c3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[14.18278985, 24.85084955, 32.26250018, ..., 10.00417662,\n",
       "          7.46888704,  3.62294237],\n",
       "        [12.57967283, 23.76741535, 30.37286871, ..., 14.95723319,\n",
       "         11.13698112,  5.84942905],\n",
       "        [13.22193417, 24.55529474, 30.39580222, ..., 13.83155641,\n",
       "         10.59001351,  5.29500675],\n",
       "        ...,\n",
       "        [13.29084536, 24.17527752, 31.31167992, ..., 14.23324907,\n",
       "         10.53464287,  4.58407225],\n",
       "        [12.95507451, 24.79821619, 31.58751678, ..., 15.15993757,\n",
       "         11.82398723,  5.91199362],\n",
       "        [12.83678016, 22.97733591, 30.31822964, ...,  9.57554661,\n",
       "          7.2255416 ,  3.23136044]]),\n",
       " array([ 3, 14,  9, ...,  4, 14, 15]),\n",
       " array([[13.28076564, 22.61852359, 31.40274702, ..., 10.60715627,\n",
       "          7.27644917,  3.63822458],\n",
       "        [12.94003695, 25.86111181, 29.316574  , ..., 10.92564205,\n",
       "          8.02663748,  4.04376155],\n",
       "        [14.43351816, 25.80163901, 33.27687417, ..., 11.81740281,\n",
       "          9.40164209,  3.67045161],\n",
       "        ...,\n",
       "        [ 9.31385356, 16.99185373, 21.98111139, ...,  8.65813213,\n",
       "          5.74999087,  2.86072738],\n",
       "        [10.12595963, 18.25930482, 23.00005503, ...,  9.45903101,\n",
       "          5.88334479,  3.65900747],\n",
       "        [ 9.184745  , 17.25962422, 21.51717653, ...,  8.96689396,\n",
       "          5.68277104,  2.87023281]]),\n",
       " array([ 0,  0,  0, ..., 15, 15, 15])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba352e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X1 = pd.DataFrame(data[0], columns=[f\"Feature_{i}\" for i in range(1, 121)])\n",
    "label1 = pd.DataFrame(data[1], columns=['Titik'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8af51a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = pd.DataFrame(data[2], columns=[f\"Feature_{i}\" for i in range(1, 121)])\n",
    "label2 = pd.DataFrame(data[3], columns=['Titik'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ac1c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_1</th>\n",
       "      <th>Feature_2</th>\n",
       "      <th>Feature_3</th>\n",
       "      <th>Feature_4</th>\n",
       "      <th>Feature_5</th>\n",
       "      <th>Feature_6</th>\n",
       "      <th>Feature_7</th>\n",
       "      <th>Feature_8</th>\n",
       "      <th>Feature_9</th>\n",
       "      <th>Feature_10</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature_111</th>\n",
       "      <th>Feature_112</th>\n",
       "      <th>Feature_113</th>\n",
       "      <th>Feature_114</th>\n",
       "      <th>Feature_115</th>\n",
       "      <th>Feature_116</th>\n",
       "      <th>Feature_117</th>\n",
       "      <th>Feature_118</th>\n",
       "      <th>Feature_119</th>\n",
       "      <th>Feature_120</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.182790</td>\n",
       "      <td>24.850850</td>\n",
       "      <td>32.262500</td>\n",
       "      <td>32.104462</td>\n",
       "      <td>32.732036</td>\n",
       "      <td>32.832134</td>\n",
       "      <td>32.252328</td>\n",
       "      <td>29.209095</td>\n",
       "      <td>26.996391</td>\n",
       "      <td>24.731724</td>\n",
       "      <td>...</td>\n",
       "      <td>14.148042</td>\n",
       "      <td>13.771947</td>\n",
       "      <td>14.992592</td>\n",
       "      <td>14.937774</td>\n",
       "      <td>14.171217</td>\n",
       "      <td>12.872921</td>\n",
       "      <td>12.259258</td>\n",
       "      <td>10.004177</td>\n",
       "      <td>7.468887</td>\n",
       "      <td>3.622942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.579673</td>\n",
       "      <td>23.767415</td>\n",
       "      <td>30.372869</td>\n",
       "      <td>31.232673</td>\n",
       "      <td>33.272799</td>\n",
       "      <td>33.410943</td>\n",
       "      <td>34.376762</td>\n",
       "      <td>31.677196</td>\n",
       "      <td>29.252769</td>\n",
       "      <td>27.177217</td>\n",
       "      <td>...</td>\n",
       "      <td>14.957233</td>\n",
       "      <td>14.385406</td>\n",
       "      <td>16.304312</td>\n",
       "      <td>17.510751</td>\n",
       "      <td>16.324479</td>\n",
       "      <td>15.655788</td>\n",
       "      <td>15.497378</td>\n",
       "      <td>14.957233</td>\n",
       "      <td>11.136981</td>\n",
       "      <td>5.849429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.221934</td>\n",
       "      <td>24.555295</td>\n",
       "      <td>30.395802</td>\n",
       "      <td>31.858168</td>\n",
       "      <td>32.579925</td>\n",
       "      <td>33.709450</td>\n",
       "      <td>34.535939</td>\n",
       "      <td>32.534333</td>\n",
       "      <td>29.670943</td>\n",
       "      <td>27.627318</td>\n",
       "      <td>...</td>\n",
       "      <td>16.264632</td>\n",
       "      <td>15.464150</td>\n",
       "      <td>15.549236</td>\n",
       "      <td>16.694961</td>\n",
       "      <td>15.453482</td>\n",
       "      <td>16.213852</td>\n",
       "      <td>15.108109</td>\n",
       "      <td>13.831556</td>\n",
       "      <td>10.590014</td>\n",
       "      <td>5.295007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.145559</td>\n",
       "      <td>22.572229</td>\n",
       "      <td>28.117823</td>\n",
       "      <td>29.045670</td>\n",
       "      <td>29.722553</td>\n",
       "      <td>30.406173</td>\n",
       "      <td>31.223747</td>\n",
       "      <td>29.476168</td>\n",
       "      <td>27.352894</td>\n",
       "      <td>26.169936</td>\n",
       "      <td>...</td>\n",
       "      <td>14.746516</td>\n",
       "      <td>14.220093</td>\n",
       "      <td>15.662222</td>\n",
       "      <td>16.706811</td>\n",
       "      <td>16.284803</td>\n",
       "      <td>15.298183</td>\n",
       "      <td>14.969623</td>\n",
       "      <td>13.154452</td>\n",
       "      <td>9.530436</td>\n",
       "      <td>4.641883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.240166</td>\n",
       "      <td>22.681091</td>\n",
       "      <td>28.016180</td>\n",
       "      <td>29.564677</td>\n",
       "      <td>30.125537</td>\n",
       "      <td>31.110680</td>\n",
       "      <td>31.323042</td>\n",
       "      <td>28.792205</td>\n",
       "      <td>25.805248</td>\n",
       "      <td>24.595152</td>\n",
       "      <td>...</td>\n",
       "      <td>16.037953</td>\n",
       "      <td>16.365292</td>\n",
       "      <td>17.175646</td>\n",
       "      <td>18.584545</td>\n",
       "      <td>16.932697</td>\n",
       "      <td>16.466251</td>\n",
       "      <td>16.037953</td>\n",
       "      <td>13.254276</td>\n",
       "      <td>10.426895</td>\n",
       "      <td>4.191370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>9.266842</td>\n",
       "      <td>17.741549</td>\n",
       "      <td>21.472756</td>\n",
       "      <td>21.884120</td>\n",
       "      <td>23.052769</td>\n",
       "      <td>23.674783</td>\n",
       "      <td>24.000391</td>\n",
       "      <td>23.117153</td>\n",
       "      <td>21.271855</td>\n",
       "      <td>19.991149</td>\n",
       "      <td>...</td>\n",
       "      <td>11.039725</td>\n",
       "      <td>10.904263</td>\n",
       "      <td>10.360644</td>\n",
       "      <td>12.191337</td>\n",
       "      <td>11.099400</td>\n",
       "      <td>11.054674</td>\n",
       "      <td>9.086886</td>\n",
       "      <td>9.769987</td>\n",
       "      <td>6.702156</td>\n",
       "      <td>2.873526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>9.926696</td>\n",
       "      <td>18.001538</td>\n",
       "      <td>22.367428</td>\n",
       "      <td>23.681516</td>\n",
       "      <td>23.066123</td>\n",
       "      <td>24.913154</td>\n",
       "      <td>25.210025</td>\n",
       "      <td>24.565636</td>\n",
       "      <td>21.485195</td>\n",
       "      <td>21.554341</td>\n",
       "      <td>...</td>\n",
       "      <td>11.558122</td>\n",
       "      <td>11.326936</td>\n",
       "      <td>12.306352</td>\n",
       "      <td>12.413365</td>\n",
       "      <td>12.292909</td>\n",
       "      <td>11.840758</td>\n",
       "      <td>10.173462</td>\n",
       "      <td>9.092150</td>\n",
       "      <td>6.556443</td>\n",
       "      <td>2.932130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>9.313854</td>\n",
       "      <td>16.991854</td>\n",
       "      <td>21.981111</td>\n",
       "      <td>22.085112</td>\n",
       "      <td>23.289905</td>\n",
       "      <td>23.184250</td>\n",
       "      <td>24.091333</td>\n",
       "      <td>23.035518</td>\n",
       "      <td>21.506887</td>\n",
       "      <td>19.860949</td>\n",
       "      <td>...</td>\n",
       "      <td>10.234849</td>\n",
       "      <td>11.739444</td>\n",
       "      <td>11.514206</td>\n",
       "      <td>12.350923</td>\n",
       "      <td>10.930824</td>\n",
       "      <td>10.314499</td>\n",
       "      <td>8.658132</td>\n",
       "      <td>8.658132</td>\n",
       "      <td>5.749991</td>\n",
       "      <td>2.860727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>10.125960</td>\n",
       "      <td>18.259305</td>\n",
       "      <td>23.000055</td>\n",
       "      <td>23.324313</td>\n",
       "      <td>24.458741</td>\n",
       "      <td>25.065357</td>\n",
       "      <td>25.587541</td>\n",
       "      <td>24.671430</td>\n",
       "      <td>22.315483</td>\n",
       "      <td>21.011628</td>\n",
       "      <td>...</td>\n",
       "      <td>11.443096</td>\n",
       "      <td>11.932038</td>\n",
       "      <td>11.095376</td>\n",
       "      <td>12.930233</td>\n",
       "      <td>12.688048</td>\n",
       "      <td>12.269358</td>\n",
       "      <td>9.459031</td>\n",
       "      <td>9.459031</td>\n",
       "      <td>5.883345</td>\n",
       "      <td>3.659007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>9.184745</td>\n",
       "      <td>17.259624</td>\n",
       "      <td>21.517177</td>\n",
       "      <td>21.730525</td>\n",
       "      <td>22.387816</td>\n",
       "      <td>23.430665</td>\n",
       "      <td>23.952260</td>\n",
       "      <td>23.140557</td>\n",
       "      <td>20.609791</td>\n",
       "      <td>20.222415</td>\n",
       "      <td>...</td>\n",
       "      <td>11.278225</td>\n",
       "      <td>10.921981</td>\n",
       "      <td>11.086681</td>\n",
       "      <td>12.836071</td>\n",
       "      <td>11.834273</td>\n",
       "      <td>12.177367</td>\n",
       "      <td>9.775661</td>\n",
       "      <td>8.966894</td>\n",
       "      <td>5.682771</td>\n",
       "      <td>2.870233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9600 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
       "0     14.182790  24.850850  32.262500  32.104462  32.732036  32.832134   \n",
       "1     12.579673  23.767415  30.372869  31.232673  33.272799  33.410943   \n",
       "2     13.221934  24.555295  30.395802  31.858168  32.579925  33.709450   \n",
       "3     12.145559  22.572229  28.117823  29.045670  29.722553  30.406173   \n",
       "4     12.240166  22.681091  28.016180  29.564677  30.125537  31.110680   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "3195   9.266842  17.741549  21.472756  21.884120  23.052769  23.674783   \n",
       "3196   9.926696  18.001538  22.367428  23.681516  23.066123  24.913154   \n",
       "3197   9.313854  16.991854  21.981111  22.085112  23.289905  23.184250   \n",
       "3198  10.125960  18.259305  23.000055  23.324313  24.458741  25.065357   \n",
       "3199   9.184745  17.259624  21.517177  21.730525  22.387816  23.430665   \n",
       "\n",
       "      Feature_7  Feature_8  Feature_9  Feature_10  ...  Feature_111  \\\n",
       "0     32.252328  29.209095  26.996391   24.731724  ...    14.148042   \n",
       "1     34.376762  31.677196  29.252769   27.177217  ...    14.957233   \n",
       "2     34.535939  32.534333  29.670943   27.627318  ...    16.264632   \n",
       "3     31.223747  29.476168  27.352894   26.169936  ...    14.746516   \n",
       "4     31.323042  28.792205  25.805248   24.595152  ...    16.037953   \n",
       "...         ...        ...        ...         ...  ...          ...   \n",
       "3195  24.000391  23.117153  21.271855   19.991149  ...    11.039725   \n",
       "3196  25.210025  24.565636  21.485195   21.554341  ...    11.558122   \n",
       "3197  24.091333  23.035518  21.506887   19.860949  ...    10.234849   \n",
       "3198  25.587541  24.671430  22.315483   21.011628  ...    11.443096   \n",
       "3199  23.952260  23.140557  20.609791   20.222415  ...    11.278225   \n",
       "\n",
       "      Feature_112  Feature_113  Feature_114  Feature_115  Feature_116  \\\n",
       "0       13.771947    14.992592    14.937774    14.171217    12.872921   \n",
       "1       14.385406    16.304312    17.510751    16.324479    15.655788   \n",
       "2       15.464150    15.549236    16.694961    15.453482    16.213852   \n",
       "3       14.220093    15.662222    16.706811    16.284803    15.298183   \n",
       "4       16.365292    17.175646    18.584545    16.932697    16.466251   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "3195    10.904263    10.360644    12.191337    11.099400    11.054674   \n",
       "3196    11.326936    12.306352    12.413365    12.292909    11.840758   \n",
       "3197    11.739444    11.514206    12.350923    10.930824    10.314499   \n",
       "3198    11.932038    11.095376    12.930233    12.688048    12.269358   \n",
       "3199    10.921981    11.086681    12.836071    11.834273    12.177367   \n",
       "\n",
       "      Feature_117  Feature_118  Feature_119  Feature_120  \n",
       "0       12.259258    10.004177     7.468887     3.622942  \n",
       "1       15.497378    14.957233    11.136981     5.849429  \n",
       "2       15.108109    13.831556    10.590014     5.295007  \n",
       "3       14.969623    13.154452     9.530436     4.641883  \n",
       "4       16.037953    13.254276    10.426895     4.191370  \n",
       "...           ...          ...          ...          ...  \n",
       "3195     9.086886     9.769987     6.702156     2.873526  \n",
       "3196    10.173462     9.092150     6.556443     2.932130  \n",
       "3197     8.658132     8.658132     5.749991     2.860727  \n",
       "3198     9.459031     9.459031     5.883345     3.659007  \n",
       "3199     9.775661     8.966894     5.682771     2.870233  \n",
       "\n",
       "[9600 rows x 120 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([X1, X2], axis=0)  # Untuk penggabungan horizontal\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67797f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Titik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Titik\n",
       "0         3\n",
       "1        14\n",
       "2         9\n",
       "3         6\n",
       "4         0\n",
       "...     ...\n",
       "3195     15\n",
       "3196     15\n",
       "3197     15\n",
       "3198     15\n",
       "3199     15\n",
       "\n",
       "[9600 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.concat([label1, label2], axis=0)  # Untuk penggabungan horizontal\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77d00df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_split, y_train, y_split = train_test_split(X, y, test_size=0.3, random_state=142, stratify=y)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_split, y_split, test_size=0.3, random_state=142, stratify=y_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd3259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_val= X_val.to_numpy()\n",
    "y_val= y_val.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74a1db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout,BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# n_features = len(X_train[0])\n",
    "# n_classes = len(set(y_train))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_dim=120, kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Dense(16, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33446a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "optimizer = Adam(learning_rate=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ac7cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb44c0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 2s 9ms/step - loss: 7.7767 - accuracy: 0.0701 - val_loss: 10.8873 - val_accuracy: 0.0625\n",
      "Epoch 2/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 7.6483 - accuracy: 0.0693 - val_loss: 7.9297 - val_accuracy: 0.0789\n",
      "Epoch 3/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 7.4954 - accuracy: 0.0900 - val_loss: 6.9461 - val_accuracy: 0.1458\n",
      "Epoch 4/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 7.3896 - accuracy: 0.1022 - val_loss: 6.6799 - val_accuracy: 0.1959\n",
      "Epoch 5/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 7.2681 - accuracy: 0.1170 - val_loss: 6.5548 - val_accuracy: 0.2153\n",
      "Epoch 6/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 7.1366 - accuracy: 0.1360 - val_loss: 6.4558 - val_accuracy: 0.2411\n",
      "Epoch 7/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 7.0633 - accuracy: 0.1463 - val_loss: 6.3736 - val_accuracy: 0.2703\n",
      "Epoch 8/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 6.9502 - accuracy: 0.1567 - val_loss: 6.2826 - val_accuracy: 0.3051\n",
      "Epoch 9/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 6.8757 - accuracy: 0.1705 - val_loss: 6.1971 - val_accuracy: 0.3279\n",
      "Epoch 10/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 6.7695 - accuracy: 0.1798 - val_loss: 6.1155 - val_accuracy: 0.3596\n",
      "Epoch 11/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 6.7044 - accuracy: 0.1935 - val_loss: 6.0319 - val_accuracy: 0.3864\n",
      "Epoch 12/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 6.6059 - accuracy: 0.2077 - val_loss: 5.9672 - val_accuracy: 0.4107\n",
      "Epoch 13/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 6.5593 - accuracy: 0.2128 - val_loss: 5.8870 - val_accuracy: 0.4335\n",
      "Epoch 14/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 6.4778 - accuracy: 0.2326 - val_loss: 5.8273 - val_accuracy: 0.4559\n",
      "Epoch 15/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 6.3910 - accuracy: 0.2464 - val_loss: 5.7668 - val_accuracy: 0.4752\n",
      "Epoch 16/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 6.3352 - accuracy: 0.2594 - val_loss: 5.7086 - val_accuracy: 0.4965\n",
      "Epoch 17/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 6.2640 - accuracy: 0.2717 - val_loss: 5.6447 - val_accuracy: 0.5169\n",
      "Epoch 18/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 6.1739 - accuracy: 0.2908 - val_loss: 5.5821 - val_accuracy: 0.5347\n",
      "Epoch 19/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 6.1274 - accuracy: 0.2917 - val_loss: 5.5207 - val_accuracy: 0.5541\n",
      "Epoch 20/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 6.0958 - accuracy: 0.3040 - val_loss: 5.4649 - val_accuracy: 0.5694\n",
      "Epoch 21/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 6.0560 - accuracy: 0.3128 - val_loss: 5.4125 - val_accuracy: 0.5809\n",
      "Epoch 22/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.9597 - accuracy: 0.3304 - val_loss: 5.3665 - val_accuracy: 0.5918\n",
      "Epoch 23/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.9436 - accuracy: 0.3237 - val_loss: 5.3169 - val_accuracy: 0.6007\n",
      "Epoch 24/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.8640 - accuracy: 0.3485 - val_loss: 5.2685 - val_accuracy: 0.6081\n",
      "Epoch 25/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.8384 - accuracy: 0.3531 - val_loss: 5.2219 - val_accuracy: 0.6200\n",
      "Epoch 26/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.7931 - accuracy: 0.3595 - val_loss: 5.1716 - val_accuracy: 0.6300\n",
      "Epoch 27/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.7477 - accuracy: 0.3680 - val_loss: 5.1261 - val_accuracy: 0.6369\n",
      "Epoch 28/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.6880 - accuracy: 0.3766 - val_loss: 5.0823 - val_accuracy: 0.6503\n",
      "Epoch 29/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.6193 - accuracy: 0.3958 - val_loss: 5.0427 - val_accuracy: 0.6558\n",
      "Epoch 30/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.5876 - accuracy: 0.4028 - val_loss: 5.0036 - val_accuracy: 0.6691\n",
      "Epoch 31/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.5529 - accuracy: 0.4067 - val_loss: 4.9674 - val_accuracy: 0.6741\n",
      "Epoch 32/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.5076 - accuracy: 0.4229 - val_loss: 4.9217 - val_accuracy: 0.6890\n",
      "Epoch 33/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.4694 - accuracy: 0.4231 - val_loss: 4.8829 - val_accuracy: 0.6989\n",
      "Epoch 34/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.4295 - accuracy: 0.4381 - val_loss: 4.8429 - val_accuracy: 0.7059\n",
      "Epoch 35/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.3891 - accuracy: 0.4470 - val_loss: 4.8063 - val_accuracy: 0.7153\n",
      "Epoch 36/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.3296 - accuracy: 0.4552 - val_loss: 4.7660 - val_accuracy: 0.7222\n",
      "Epoch 37/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.3432 - accuracy: 0.4518 - val_loss: 4.7341 - val_accuracy: 0.7272\n",
      "Epoch 38/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.2830 - accuracy: 0.4668 - val_loss: 4.6972 - val_accuracy: 0.7331\n",
      "Epoch 39/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 5.2594 - accuracy: 0.4698 - val_loss: 4.6625 - val_accuracy: 0.7416\n",
      "Epoch 40/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.1876 - accuracy: 0.4909 - val_loss: 4.6296 - val_accuracy: 0.7520\n",
      "Epoch 41/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.1776 - accuracy: 0.4915 - val_loss: 4.6023 - val_accuracy: 0.7540\n",
      "Epoch 42/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.1243 - accuracy: 0.5000 - val_loss: 4.5754 - val_accuracy: 0.7624\n",
      "Epoch 43/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.0863 - accuracy: 0.5046 - val_loss: 4.5460 - val_accuracy: 0.7703\n",
      "Epoch 44/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.0412 - accuracy: 0.5196 - val_loss: 4.5064 - val_accuracy: 0.7852\n",
      "Epoch 45/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.0142 - accuracy: 0.5308 - val_loss: 4.4762 - val_accuracy: 0.7907\n",
      "Epoch 46/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 5.0148 - accuracy: 0.5231 - val_loss: 4.4477 - val_accuracy: 0.7996\n",
      "Epoch 47/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.9839 - accuracy: 0.5332 - val_loss: 4.4142 - val_accuracy: 0.8085\n",
      "Epoch 48/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.9424 - accuracy: 0.5374 - val_loss: 4.3789 - val_accuracy: 0.8160\n",
      "Epoch 49/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.9056 - accuracy: 0.5415 - val_loss: 4.3541 - val_accuracy: 0.8229\n",
      "Epoch 50/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 4.8698 - accuracy: 0.5525 - val_loss: 4.3278 - val_accuracy: 0.8274\n",
      "Epoch 51/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.8443 - accuracy: 0.5583 - val_loss: 4.2956 - val_accuracy: 0.8343\n",
      "Epoch 52/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 4.8222 - accuracy: 0.5610 - val_loss: 4.2653 - val_accuracy: 0.8383\n",
      "Epoch 53/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 4.8053 - accuracy: 0.5580 - val_loss: 4.2381 - val_accuracy: 0.8447\n",
      "Epoch 54/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 4.7616 - accuracy: 0.5705 - val_loss: 4.2107 - val_accuracy: 0.8487\n",
      "Epoch 55/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.7299 - accuracy: 0.5792 - val_loss: 4.1868 - val_accuracy: 0.8507\n",
      "Epoch 56/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 4.6913 - accuracy: 0.5848 - val_loss: 4.1575 - val_accuracy: 0.8576\n",
      "Epoch 57/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.6663 - accuracy: 0.5967 - val_loss: 4.1343 - val_accuracy: 0.8621\n",
      "Epoch 58/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.6410 - accuracy: 0.6009 - val_loss: 4.1083 - val_accuracy: 0.8626\n",
      "Epoch 59/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 4.6137 - accuracy: 0.6039 - val_loss: 4.0788 - val_accuracy: 0.8686\n",
      "Epoch 60/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 4.5848 - accuracy: 0.6049 - val_loss: 4.0564 - val_accuracy: 0.8745\n",
      "Epoch 61/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 4.5575 - accuracy: 0.6149 - val_loss: 4.0327 - val_accuracy: 0.8750\n",
      "Epoch 62/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.5257 - accuracy: 0.6222 - val_loss: 4.0067 - val_accuracy: 0.8780\n",
      "Epoch 63/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.5136 - accuracy: 0.6229 - val_loss: 3.9801 - val_accuracy: 0.8790\n",
      "Epoch 64/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.4804 - accuracy: 0.6274 - val_loss: 3.9614 - val_accuracy: 0.8834\n",
      "Epoch 65/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.4600 - accuracy: 0.6344 - val_loss: 3.9383 - val_accuracy: 0.8854\n",
      "Epoch 66/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.4307 - accuracy: 0.6375 - val_loss: 3.9133 - val_accuracy: 0.8934\n",
      "Epoch 67/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.4200 - accuracy: 0.6363 - val_loss: 3.8884 - val_accuracy: 0.8968\n",
      "Epoch 68/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 4.3655 - accuracy: 0.6583 - val_loss: 3.8635 - val_accuracy: 0.9013\n",
      "Epoch 69/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.3633 - accuracy: 0.6567 - val_loss: 3.8401 - val_accuracy: 0.9043\n",
      "Epoch 70/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.3332 - accuracy: 0.6582 - val_loss: 3.8167 - val_accuracy: 0.9067\n",
      "Epoch 71/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.3089 - accuracy: 0.6664 - val_loss: 3.7947 - val_accuracy: 0.9107\n",
      "Epoch 72/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.2715 - accuracy: 0.6716 - val_loss: 3.7771 - val_accuracy: 0.9127\n",
      "Epoch 73/100\n",
      "56/56 [==============================] - 0s 6ms/step - loss: 4.2537 - accuracy: 0.6744 - val_loss: 3.7507 - val_accuracy: 0.9147\n",
      "Epoch 74/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.2210 - accuracy: 0.6881 - val_loss: 3.7273 - val_accuracy: 0.9177\n",
      "Epoch 75/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.2074 - accuracy: 0.6887 - val_loss: 3.7088 - val_accuracy: 0.9201\n",
      "Epoch 76/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 4.1855 - accuracy: 0.6906 - val_loss: 3.6812 - val_accuracy: 0.9256\n",
      "Epoch 77/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.1680 - accuracy: 0.6899 - val_loss: 3.6591 - val_accuracy: 0.9276\n",
      "Epoch 78/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 4.1345 - accuracy: 0.6967 - val_loss: 3.6375 - val_accuracy: 0.9311\n",
      "Epoch 79/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.1179 - accuracy: 0.7091 - val_loss: 3.6212 - val_accuracy: 0.9320\n",
      "Epoch 80/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.0912 - accuracy: 0.7110 - val_loss: 3.6046 - val_accuracy: 0.9350\n",
      "Epoch 81/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.0739 - accuracy: 0.7091 - val_loss: 3.5802 - val_accuracy: 0.9405\n",
      "Epoch 82/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.0470 - accuracy: 0.7164 - val_loss: 3.5625 - val_accuracy: 0.9425\n",
      "Epoch 83/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 4.0295 - accuracy: 0.7155 - val_loss: 3.5479 - val_accuracy: 0.9410\n",
      "Epoch 84/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.9853 - accuracy: 0.7292 - val_loss: 3.5260 - val_accuracy: 0.9430\n",
      "Epoch 85/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.9777 - accuracy: 0.7259 - val_loss: 3.5021 - val_accuracy: 0.9459\n",
      "Epoch 86/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.9703 - accuracy: 0.7246 - val_loss: 3.4830 - val_accuracy: 0.9469\n",
      "Epoch 87/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.9395 - accuracy: 0.7350 - val_loss: 3.4640 - val_accuracy: 0.9499\n",
      "Epoch 88/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.9185 - accuracy: 0.7390 - val_loss: 3.4471 - val_accuracy: 0.9514\n",
      "Epoch 89/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.9075 - accuracy: 0.7382 - val_loss: 3.4285 - val_accuracy: 0.9534\n",
      "Epoch 90/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 3.8814 - accuracy: 0.7397 - val_loss: 3.4107 - val_accuracy: 0.9529\n",
      "Epoch 91/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.8699 - accuracy: 0.7356 - val_loss: 3.3894 - val_accuracy: 0.9578\n",
      "Epoch 92/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.8444 - accuracy: 0.7500 - val_loss: 3.3714 - val_accuracy: 0.9583\n",
      "Epoch 93/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.8268 - accuracy: 0.7496 - val_loss: 3.3524 - val_accuracy: 0.9598\n",
      "Epoch 94/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.8031 - accuracy: 0.7463 - val_loss: 3.3378 - val_accuracy: 0.9618\n",
      "Epoch 95/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.7773 - accuracy: 0.7585 - val_loss: 3.3202 - val_accuracy: 0.9613\n",
      "Epoch 96/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.7669 - accuracy: 0.7604 - val_loss: 3.3026 - val_accuracy: 0.9628\n",
      "Epoch 97/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.7476 - accuracy: 0.7609 - val_loss: 3.2904 - val_accuracy: 0.9653\n",
      "Epoch 98/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 3.7141 - accuracy: 0.7702 - val_loss: 3.2682 - val_accuracy: 0.9658\n",
      "Epoch 99/100\n",
      "56/56 [==============================] - 0s 4ms/step - loss: 3.6971 - accuracy: 0.7702 - val_loss: 3.2487 - val_accuracy: 0.9668\n",
      "Epoch 100/100\n",
      "56/56 [==============================] - 0s 5ms/step - loss: 3.6845 - accuracy: 0.7741 - val_loss: 3.2319 - val_accuracy: 0.9668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22f9908a820>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.fit(X_train, y_train, epochs=100, batch_size=120, validation_data=(X_val, y_val),callbacks=[my_callback])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=120, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d9c229f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95        54\n",
      "           1       1.00      0.96      0.98        54\n",
      "           2       1.00      0.98      0.99        54\n",
      "           3       0.98      0.93      0.95        54\n",
      "           4       0.96      1.00      0.98        54\n",
      "           5       0.98      0.98      0.98        54\n",
      "           6       0.95      1.00      0.97        54\n",
      "           7       1.00      1.00      1.00        54\n",
      "           8       0.96      0.94      0.95        54\n",
      "           9       0.98      0.98      0.98        54\n",
      "          10       0.94      0.94      0.94        54\n",
      "          11       1.00      0.96      0.98        54\n",
      "          12       0.94      0.85      0.89        54\n",
      "          13       0.92      0.91      0.92        54\n",
      "          14       0.98      1.00      0.99        54\n",
      "          15       0.90      1.00      0.95        54\n",
      "\n",
      "    accuracy                           0.96       864\n",
      "   macro avg       0.96      0.96      0.96       864\n",
      "weighted avg       0.96      0.96      0.96       864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Menghitung classification report\n",
    "report = classification_report(y_test, y_pred_classes)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60105818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 0s 2ms/step - loss: 3.2464 - accuracy: 0.9641\n",
      "Akurasi val: 96.41%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Akurasi val: {accuracy*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
